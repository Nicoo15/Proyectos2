{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMJc2E582CY4"
   },
   "source": [
    "## Instalación de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZNDS66gkUINw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "El sistema no puede encontrar la ruta especificada.\n",
      "\"wget\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\"tar\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
    "!tar xf spark-3.3.0-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVFNlaX02O1Q"
   },
   "source": [
    "### Cargamos el entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AN6dEAkqUYa9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1CWOgdWOi6Ii"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFOEq07t2hTw"
   },
   "source": [
    "### Importamos pyspark y creamos una sessión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w3eV8P9EUbIZ"
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Unable to find py4j in spark-3.3.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\findspark.py:159\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     py4j \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark_python\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlib\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpy4j-*.zip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfindspark\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mfindspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark-3.3.0-bin-hadoop3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m# SPARK_HOME\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      4\u001b[0m ss \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal[*]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\findspark.py:161\u001b[0m, in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    159\u001b[0m         py4j \u001b[38;5;241m=\u001b[39m glob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(spark_python, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlib\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpy4j-*.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find py4j in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, your SPARK_HOME may not be configured correctly\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    163\u001b[0m                 spark_python\n\u001b[0;32m    164\u001b[0m             )\n\u001b[0;32m    165\u001b[0m         )\n\u001b[0;32m    166\u001b[0m     sys\u001b[38;5;241m.\u001b[39mpath[:\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m sys_path \u001b[38;5;241m=\u001b[39m [spark_python, py4j]\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# already imported, no need to patch sys.path\u001b[39;00m\n",
      "\u001b[1;31mException\u001b[0m: Unable to find py4j in spark-3.3.0-bin-hadoop3\\python, your SPARK_HOME may not be configured correctly"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"spark-3.3.0-bin-hadoop3\")# SPARK_HOME\n",
    "from pyspark.sql import SparkSession\n",
    "ss = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBaBEPuz2jYl"
   },
   "source": [
    "## Nuestros primer conjunto de datos en un cluster\n",
    "Creamos una lista en local y la subimos al cluster\n",
    "\n",
    "Los datos se almacenan en una estructura de datos distribuida denominada [RDD](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Pr_YhhZachu"
   },
   "outputs": [],
   "source": [
    "data = ['apple', 'orange', 'banana', 'grape', 'watermelon', 'apple', 'orange', 'apple']\n",
    "# Subimos los datos al cluster\n",
    "distData = ss.sparkContext.parallelize(data)\n",
    "distData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO3LYjp87nzd"
   },
   "source": [
    "### Operaciones básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBG3-r-87rPn"
   },
   "outputs": [],
   "source": [
    "# Contar número de elementos/filas\n",
    "distData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNds6SwI80Ql"
   },
   "source": [
    "#### Obtener elementos\n",
    "Obtener el/la primer/a elemento/fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvKqeqxm7wwd"
   },
   "outputs": [],
   "source": [
    "distData.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9bokUrn92jv"
   },
   "source": [
    "Obtener x primeros elementos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQuez8uh9qVr"
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "print(distData.take(x))\n",
    "x = 3\n",
    "print(distData.take(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r3Ncd3RN95GH"
   },
   "source": [
    "Obtener x primeros elementos por orden ASCII"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpRBCcte9qM_"
   },
   "outputs": [],
   "source": [
    "distData.takeOrdered(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_4N2ZNY98zS"
   },
   "source": [
    "Obtener todos los elementos. \n",
    "\n",
    "Cuidado, trae toda la información del conjunto de datos distribuido en el cluster a la máquina local pudiendo producir un desbordamiento de la memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7soQi43_9qCh"
   },
   "outputs": [],
   "source": [
    "distData.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qIdYbpS17G2u"
   },
   "source": [
    "#### Transformaciones básicas\n",
    "Tenemos a nuestra disposición los métodos de programación funcional típicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdC2Aebd_LS5"
   },
   "outputs": [],
   "source": [
    "dir(distData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFh8oMja_E-F"
   },
   "source": [
    "Utiliza `map` para concatenar la palabra fruit a cada fruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns3qdv0U_wsn"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "fruits = distData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pkVylC5_3GT"
   },
   "source": [
    "Cuenta el número de caracteres de cada cadena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l2PLqxiPAF98"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "distData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbJDeAC8A1Om"
   },
   "source": [
    "Cuenta el número total de caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "59xf9E8-A07H"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "distData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDSRVl6SIRwo"
   },
   "source": [
    "Obtener los distintos tipos de fruta existentes\n",
    "\n",
    "Utilizar `distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BEFy1pqIqAD"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "distData."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6Rr3L_7yghO"
   },
   "source": [
    "### Pair RDD\n",
    "\n",
    "Es un tipo de estructura de datos particular que permite trabajar pares de clave-valor donde realizar [operaciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#working-with-key-value-pairs) que impliquen agrupamiento por clave.\n",
    "\n",
    "Podemos crear un `RDD` de tipo Pair a partir del conjunto de datos anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbUIDcaSyo55"
   },
   "outputs": [],
   "source": [
    "pairRDD = distData.map(lambda x: (x, 1))\n",
    "pairRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26gwxURoz3-i"
   },
   "source": [
    "Obtener los distintos tipos de fruta existentes\n",
    "\n",
    "Ahora utilizar `groupByKey()` y guardar el resultado en `groupedRDD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KWPLashGz_Pg"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "groupedRDD = \n",
    "groupedRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aG0oJt5C6NcH"
   },
   "source": [
    "`groupedRDD` contiene un listado con las distintas claves existentes en el `pairRDD` y un iterador a los valores de cada clave. \n",
    "\n",
    "Podemos recopilar todos esos elmentos aplicando la función `list` a cada iterador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86tk8OdP6fkb"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "groupedRDD.mapValues("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5SfwEoe1-t0"
   },
   "source": [
    "Contar número de veces que aparece cada fruta.\n",
    "\n",
    "Utilizar `reduceByKey()` con `pairRDD`\n",
    "\n",
    "y \n",
    "\n",
    "`mapValues` con `groupedRDD`\n",
    "\n",
    "¿Cuál es más eficiente?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwqNEXho8y5d"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "pairRDD.reduceByKey("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDcXJWwr8shh"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "groupedRDD.mapValues("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5pEA8kwJfX_"
   },
   "source": [
    "Tambien se puede hacer con `countByKey()`\n",
    "Cuidado para conjuntos de datos grandes cuyo resultado no es posible guardar en local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPKPLwWXJmYJ"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "pairRDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4450Fdi0-FU6"
   },
   "source": [
    "Cuenta el número de apariciones de cada fruta a partir de `distData`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4W21WFs29Yua"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yb4Tk3rN-t_j"
   },
   "source": [
    "Encuentra aquellas frutas que solo aparecen una vez.\n",
    "\n",
    "Utilizar `filter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBfdc1ne9Ynq"
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gJp_L54DAZR"
   },
   "source": [
    "## EJERCICIO\n",
    "Apliquemos lo aprendido sobre el archivo `shakespeare.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3TGr-yb19vp"
   },
   "outputs": [],
   "source": [
    "fileRDD = ss.sparkContext.textFile(\"/content/shakespeare.txt\")\n",
    "fileRDD.takeSample(False, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGEf1TmVErL3"
   },
   "source": [
    "#### Obtener las 10 palabras que aparecen con más frecuencia y su frecuencia absoluta\n",
    "\n",
    "Algunas funciones útiles que no has utilizado aún\n",
    "\n",
    "\n",
    "*   `flatMap`\n",
    "*   `split`\n",
    "\n",
    "Ten en cuenta que las palabras pueden tener letras mayusculas que hacen que se identifiquen como palabras diferentes. Utiliza la funcion `lower` the `str` para poner todas las palabras en letras minusculas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dxZPTWEfE8eY"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "diez_mas_frecuentes = fileRDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKfMBEW4nBtx"
   },
   "source": [
    "#### Extra: generar histograma con el recuento de las diez palabras más frecuentas.\n",
    "Utilizar `barplot` de Seaborn. \n",
    "Seaborn espera un `DataFrame` de Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ne2MwNOHjNX3"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(diez_mas_frecuentes_sol, columns =['word', 'count'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XzedekLoV_j"
   },
   "outputs": [],
   "source": [
    "#TODO\n",
    "plt = sns.barplot("
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1.2.1 Intro pySpark_alumnos.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
